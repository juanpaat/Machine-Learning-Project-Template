{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOYJDEdcFgHB5UWGwSU4/ax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanpaat/Machine-Learning-Project-Template/blob/main/Template_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Load/Create the data"
      ],
      "metadata": {
        "id": "Al66S_7ELgNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Dataset"
      ],
      "metadata": {
        "id": "yZe3X_CVHYIq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e902VVyDFRt2"
      },
      "outputs": [],
      "source": [
        "# Creating a manual dataset\n",
        "data = pd.DataFrame({\n",
        "    'name': ['John', 'Jane', 'Jack', 'John', None],\n",
        "    'age': [28, 34, None, 28, 22],\n",
        "    'purchase_amount': [100.5, None, 85.3, 100.5, 50.0],\n",
        "    'date_of_purchase': ['2023/12/01', '2023/12/02', '2023/12/01', '2023/12/01', '2023/12/03']\n",
        "\t})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read CVS Files"
      ],
      "metadata": {
        "id": "wUek6Bx_IhwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"data/sheffield_weather_station.csv\",  # Replace with your CSV file path\n",
        "    # The following arguments are optional and can be removed:\n",
        "    # If columns aren't separated by commas, indicate the delimiter here\n",
        "    sep=\"\\s+\",\n",
        "    # Indicate which zero-indexed row number(s) have the column names\n",
        "    header=0,\n",
        "    # List of column names to use (useful for renaming columns)\n",
        "    names=[\"year\", \"month\", \"max_c\", \"min_c\", \"af\", \"rain\", \"sun\"],\n",
        "    # If not all columns are needed, indicate which you need (useful for lower memory usage)\n",
        "    usecols=[\"year\", \"month\", \"max_c\", \"min_c\", \"rain\", \"sun\"],\n",
        "    # Indicate which column(s) to use as row labels\n",
        "    index_col=[\"year\", \"month\"],\n",
        "    # Lines starting with this string should be ignored (useful if there are file comments)\n",
        "    comment=\"#\",\n",
        "    # Indicate the number of lines to skip at the start of the file (also useful for file comments)\n",
        "    skiprows=None,\n",
        "    # Indicate string(s) that should be recognized as NaN/NA\n",
        "    na_values=[\"---\", \"unknown\", \"no info\"],\n",
        "    # Indicate which column(s) are date column(s)\n",
        "    parse_dates=False,\n",
        "    # Indicate number of rows to read (useful for large files)\n",
        "    nrows=500,\n",
        "    # Encoding to use when reading file\n",
        "    encoding=\"utf-8\",\n",
        ")"
      ],
      "metadata": {
        "id": "ZXteCsUeIhG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Excel Files"
      ],
      "metadata": {
        "id": "ZiSR2eHlIt-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\n",
        "    \"data/employee_information.xlsx\",  # Replace with your Excel file path\n",
        "    # The following arguments are optional and can be removed:\n",
        "    # By default pandas will only read the first sheet\n",
        "    # You can change that by specifying the sheet name(s) or zero-indexed sheet position(s)\n",
        "    sheet_name=\"Employee Addresses\",\n",
        "    # Indicate which zero-indexed row number(s) have the column names\n",
        "    header=1,\n",
        "    # If not all columns are needed, indicate which you need (useful for lower memory usage)\n",
        "    usecols=(0, 1, 3, 4, 5, 6),\n",
        "    # List of column names to use (useful for renaming columns)\n",
        "    names=[\"id\", \"lastname\", \"country\", \"city\", \"street\", \"number\"],\n",
        "    # Indicate which column(s) to use as row labels\n",
        "    index_col=\"id\",\n",
        "    # Lines starting with this string should be ignored (useful if there are file comments)\n",
        "    comment=\"Last updated:\",\n",
        "    # Indicate the number of lines to skip at the start of the file (also useful for file comments)\n",
        "    skiprows=1,\n",
        "    # Indicate string(s) that should be recognized as NaN/NA\n",
        "    na_values=[\"---\", \"unknown\", \"no info\"],\n",
        "    # Indicate which column(s) are date column(s)\n",
        "    parse_dates=False,\n",
        "    # Indicate number of rows to read (useful for large files)\n",
        "    nrows=500,\n",
        ")"
      ],
      "metadata": {
        "id": "K2xQ80QoIxq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Understand the data"
      ],
      "metadata": {
        "id": "wtXozIa1LoYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shape of a dataset"
      ],
      "metadata": {
        "id": "bWoRueZOJRhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "dxBEBo8sJUGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data description"
      ],
      "metadata": {
        "id": "v1xplV5iKJ_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.describe()"
      ],
      "metadata": {
        "id": "KlNjvfRFKLFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of observations per group"
      ],
      "metadata": {
        "id": "PCxzKg2iKwdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "SGYXYxhZKwXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outliers"
      ],
      "metadata": {
        "id": "y-TDbpcweQjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats"
      ],
      "metadata": {
        "id": "jd8cFAX1eQKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def extract_outliers_from_boxplot(series: pd.Series):\n",
        "    \"\"\"Extract outliers from a Pandas Series based on the IQR method.\"\"\"\n",
        "\n",
        "    # Ensure input is a Pandas Series\n",
        "    if not isinstance(series, pd.Series):\n",
        "        raise TypeError(\"Input must be a Pandas Series\")\n",
        "\n",
        "    # Calculate IQR (Interquartile Range)\n",
        "    iqr_q1 = series.quantile(0.25)\n",
        "    iqr_q3 = series.quantile(0.75)\n",
        "    med = series.median()\n",
        "\n",
        "    # Finding the IQR region\n",
        "    iqr = iqr_q3 - iqr_q1\n",
        "\n",
        "    # Finding upper and lower whiskers\n",
        "    upper_bound = iqr_q3 + (1.5 * iqr)\n",
        "    lower_bound = iqr_q1 - (1.5 * iqr)\n",
        "\n",
        "    # Extract outliers\n",
        "    outliers = series[(series <= lower_bound) | (series >= upper_bound)]\n",
        "\n",
        "    print(f'Outliers within the box plot are: {outliers.values}')\n",
        "    return outliers"
      ],
      "metadata": {
        "id": "dqTJdlpxm2u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_outliers_from_boxplot(array):\n",
        "    ## Get IQR\n",
        "    iqr_q1 = np.quantile(array, 0.25)\n",
        "    iqr_q3 = np.quantile(array, 0.75)\n",
        "    med = np.median(array)\n",
        "\n",
        "    # finding the iqr region\n",
        "    iqr = iqr_q3-iqr_q1\n",
        "\n",
        "    # finding upper and lower whiskers\n",
        "    upper_bound = iqr_q3+(1.5*iqr)\n",
        "    lower_bound = iqr_q1-(1.5*iqr)\n",
        "\n",
        "    outliers = array[(array <= lower_bound) | (array >= upper_bound)]\n",
        "    print('Outliers within the box plot are :{}'.format(outliers))\n",
        "    return outliers\n",
        "\n",
        "extract_outliers_from_boxplot(df['purchases'])"
      ],
      "metadata": {
        "id": "HAOZDRmYebbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Box and violin plots"
      ],
      "metadata": {
        "id": "HBz62Q9JeobI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.violinplot(df['purchases'])\n",
        "sns.boxplot(df['purchases'])"
      ],
      "metadata": {
        "id": "C13oRAoYen1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Winsorize  \n",
        "\n",
        "Replace the outlier with a threshold value"
      ],
      "metadata": {
        "id": "d2YO4p0ne5rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def winsorize(df, column, upper, lower):\n",
        "    \"\"\"\n",
        "    Apply Winsorization to a specified column in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        The input DataFrame containing the data.\n",
        "    column : str\n",
        "        The name of the column to apply Winsorization on.\n",
        "    upper : float\n",
        "        The upper percentile threshold (0-100). Values above this percentile\n",
        "        will be replaced with the percentile value.\n",
        "    lower : float\n",
        "        The lower percentile threshold (0-100). Values below this percentile\n",
        "        will be replaced with the percentile value.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        The DataFrame with the specified column Winsorized.\n",
        "    \"\"\"\n",
        "\n",
        "    col_df = df[column]\n",
        "\n",
        "    perc_upper = np.percentile(df[column],upper)\n",
        "    perc_lower = np.percentile(df[column],lower)\n",
        "\n",
        "    df[column] = np.where(df[column] >= perc_upper,\n",
        "                          perc_upper,\n",
        "                          df[column])\n",
        "\n",
        "    df[column] = np.where(df[column] <= perc_lower,\n",
        "                          perc_lower,\n",
        "                          df[column])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Example\n",
        "winsorize(df, 'purchases', 97.5, 0.025)"
      ],
      "metadata": {
        "id": "sZ6YKOvTe6Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Z-Scores  \n",
        "\n",
        "Similar method to boxplots but we can specify the percentile we want to use, to classify a point as an outlier."
      ],
      "metadata": {
        "id": "H4Vttv8bgYF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats\n",
        "\n",
        "\n",
        "purchases = df['purchases']\n",
        "\n",
        "def percentile_outliers(array,\n",
        "                        lower_bound_perc,\n",
        "                        upper_bound_perc):\n",
        "\n",
        "    upper_bound = np.percentile(df['purchases'], upper_bound_perc)\n",
        "    lower_bound = np.percentile(df['purchases'], lower_bound_perc)\n",
        "\n",
        "    outliers = array[(array <= lower_bound) | (array >= upper_bound)]\n",
        "\n",
        "    return outliers\n",
        "\n",
        "def z_score_outliers(array,\n",
        "                     z_score_lower,\n",
        "                     z_score_upper):\n",
        "\n",
        "    z_scores = scipy.stats.zscore(array)\n",
        "    outliers = (z_scores > 1.96) | (z_scores < -1.96)\n",
        "\n",
        "    return array[outliers]\n",
        "\n",
        "\n",
        "\n",
        "# example\n",
        "outliers = percentile_outliers(df['purchases'],\n",
        "               upper_bound_perc = 99,\n",
        "               lower_bound_perc = 1)\n",
        "\n",
        "\n",
        "# Example\n",
        "z_score_outliers(df['purchases'],\n",
        "                 z_score_lower = -1.96,\n",
        "                 z_score_upper = 1.96)"
      ],
      "metadata": {
        "id": "MIU4Dl6HgmvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove"
      ],
      "metadata": {
        "id": "6So9PiK4gI_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def z_score_removal(df, column, lower_z_score, upper_z_score):\n",
        "\n",
        "    col_df = df[column]\n",
        "\n",
        "    z_scores = scipy.stats.zscore(purchases)\n",
        "    outliers = (z_scores > upper_z_score) | (z_scores < lower_z_score)\n",
        "    return df[~outliers]\n",
        "\n",
        "def percentile_removal(df, column, lower_bound_perc, upper_bound_perc):\n",
        "\n",
        "    col_df = df[column]\n",
        "\n",
        "    upper_bound = np.percentile(col_df, upper_bound_perc)\n",
        "    lower_bound = np.percentile(col_df, lower_bound_perc)\n",
        "\n",
        "    z_scores = scipy.stats.zscore(purchases)\n",
        "    outliers = (z_scores > upper_bound) | (z_scores < lower_bound)\n",
        "    return df[~outliers]\n",
        "\n",
        "\n",
        "filtered_df = z_score_removal(df, 'purchases', -1.96, 1.96)\n",
        "percentile_removal(df, 'purchases', lower_bound_perc = 1, upper_bound_perc = 99)"
      ],
      "metadata": {
        "id": "Q-JHK573gI20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Plots\n"
      ],
      "metadata": {
        "id": "dIlMbZiuiHZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Variable Plots\n"
      ],
      "metadata": {
        "id": "Yw4_c8UViLuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "NpzA3Je_imlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Histograms\n"
      ],
      "metadata": {
        "id": "ZfXF5w4XiTqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.variable.hist(bins = 100)\n",
        "\n",
        "plt.hist(df_agg['variable'])\n",
        "\n",
        "df_agg['variable'].hist()"
      ],
      "metadata": {
        "id": "oRuaFRdaipPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bar charts"
      ],
      "metadata": {
        "id": "7-RAY7DFjNGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get count of number of observations by variable\n",
        "rev_values = df_agg['variable'].value_counts()\n",
        "rev_values.plot.bar()"
      ],
      "metadata": {
        "id": "bgsx741wjMq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Box Plots\n"
      ],
      "metadata": {
        "id": "EayldlnYiUr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(df_agg['variable'])"
      ],
      "metadata": {
        "id": "BR6iBlmmi9At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relationships & Multi-variable plots\n"
      ],
      "metadata": {
        "id": "BxIZEBH3iYoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scatterplots\n"
      ],
      "metadata": {
        "id": "O5cevc7RieCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create plot with matplotlib\n",
        "plt.scatter(df['Average percentage viewed (%)'] ,df['CPM (USD)'])\n",
        "\n",
        "#let's do the same thing with seaborn so we can see a trendline.\n",
        "sns.regplot(x='Average percentage viewed (%)', y ='CPM (USD)', data = df)"
      ],
      "metadata": {
        "id": "K3ub_DCsjeZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation Matrices\n"
      ],
      "metadata": {
        "id": "3yBZXpP8ifGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df.corr()\n",
        "\n",
        "sns.heatmap(corr)"
      ],
      "metadata": {
        "id": "VQ0GztdVjwKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A better example (formatting used in below chart) - https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
        "\n",
        "sns.set_theme(style=\"white\")\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr = df.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle (otherwise this looks like the square we had above and is redundant)\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "# Generate a custom diverging colormap (choose colors here)\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, annot_kws={\"fontsize\":8})\n",
        "\n",
        "#obviously many of thes variables are HIGHLY correlated. Something we may want to explore is why Average percentage viewed is negatively related to RPM"
      ],
      "metadata": {
        "id": "7GOcOJnUj2oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pivot Tables\n"
      ],
      "metadata": {
        "id": "hODP1UysigDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pivot table to explore values\n",
        "\n",
        "pd.pivot_table(df_agg_country_sub, index = 'Country Code', values = 'Average View Percentage')\n",
        "\n",
        "# more complex\n",
        "pd.pivot_table(df_agg_country_sub, index = 'Country Code', columns = 'Is Subscribed',values = 'Average View Percentage')\n",
        "\n",
        "\n",
        "#create new pivot tables\n",
        "rm_x_date = pd.pivot_table(df_ts, index='Month_Year',values = 'User Subscriptions Removed', aggfunc ='sum').reset_index()"
      ],
      "metadata": {
        "id": "KPeV1YyXkEI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bar Charts\n"
      ],
      "metadata": {
        "id": "jY6L36brihVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's plot a slightly simpler graph. Let's just look at if subscribers or non-subsribers watch my videos for longer\n",
        "pd.pivot_table(df_agg_country_sub, index = 'Is Subscribed', values = 'Average View Percentage').plot.bar()"
      ],
      "metadata": {
        "id": "Ebx1MgWckUl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Line Charts"
      ],
      "metadata": {
        "id": "iDWdd2cmiiUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First we need to make sure our date field is in the date time format. We do this by converting our string to datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "#first, we have to aggregate these by a variable. We do this with a pivot table.\n",
        "rm_x_date = pd.pivot_table(df, index = 'Date',values = 'User Subscriptions Removed', aggfunc ='sum').reset_index()\n",
        "\n",
        "#next we visualize this data with seaborn\n",
        "sns.lineplot(data=rm_x_date,x='Date', y='User Subscriptions Removed')\n",
        "\n",
        "# One can compare over months instead of days.\n",
        "df_ts['Month_Year'] = df_ts['Date'].dt.to_period('M')\n",
        "\n",
        "\"\"\"\n",
        "B: business day frequency\n",
        "D: calendar day frequency\n",
        "W: weekly frequency\n",
        "M: monthly frequency\n",
        "Q: quarterly frequency\n",
        "Y: yearly frequency\n",
        "h: hourly frequency\n",
        "min: minutely frequency\n",
        "s: secondly frequency\n",
        "ms: milliseconds\n",
        "us: microseconds\n",
        "ns: nanoseconds\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4Gk_166VkdEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Clean the data"
      ],
      "metadata": {
        "id": "jG8d2RO1LstK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rename columns"
      ],
      "metadata": {
        "id": "gbPJ5vNlLx9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns for consistency\n",
        "df.rename(columns={'Cust Name': 'Customer_Name', 'Amt': 'Amount'}, inplace=True)\n"
      ],
      "metadata": {
        "id": "222fjYvbLx1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling missing values"
      ],
      "metadata": {
        "id": "qFMXaCvgHdGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Null Values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "Fc4h5sjObBVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nulls_summary_table(df):\n",
        "    \"\"\"\n",
        "    Returns a summary table showing null value counts and percentage\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): Dataframe to check\n",
        "\n",
        "    Returns:\n",
        "    null_values (DataFrame)\n",
        "    \"\"\"\n",
        "    null_values = pd.DataFrame(df.isnull().sum())\n",
        "    null_values[1] = null_values[0]/len(df)\n",
        "    null_values.columns = ['null_count','null_pct']\n",
        "    return null_values\n",
        "\n",
        "nulls_summary_table(df)"
      ],
      "metadata": {
        "id": "2QbXhxq0bHXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean/Median/Mode Imputation\n"
      ],
      "metadata": {
        "id": "L_lSpQu9cFDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values using mean imputation for 'age' and 'purchase_amount'\n",
        "imputer = SimpleImputer(strategy='mean') # \"median\" / \"median\" / \"most_frequent\" / \"constant\"\n",
        "data[['age', 'purchase_amount']] = imputer.fit_transform(data[['age', 'purchase_amount']])\n",
        "\n",
        "\n",
        "# Fill missing values with a constant 0\n",
        "df['Amount'].fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "oi8sBIFqHeCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Mean\n",
        "X_train_m.loc[:,'age'] = X_train_m['age'].fillna(np.mean(X_train_m['age']))\n",
        "X_test_m.loc[:,'age'] = X_test_m['age'].fillna(np.mean(X_train_m['age'])) ## Cannot use training dataset to impute\n",
        "\n",
        "\n",
        "X_train_m.loc[:,'days_on_platform'] = X_train_m['days_on_platform'].fillna(np.mean(X_train_m['days_on_platform']))\n",
        "X_test_m.loc[:,'days_on_platform'] = X_test_m['days_on_platform'].fillna(np.mean(X_train_m['days_on_platform'])) ## Cannot use training dataset to impute\n",
        "\n",
        "## Median\n",
        "m_df.loc[:,'age'] = df['age'].fillna(np.median(m_df['age']))\n",
        "\n",
        "## Mode\n",
        "m_df.loc[:,'age'] = m_df['age'].fillna(stats.mode(m_df['age'])[0][0])"
      ],
      "metadata": {
        "id": "iV7WwZjobb9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Imputation Using Regression"
      ],
      "metadata": {
        "id": "BHWOXt-ib0JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "## Target - Purchases in the first six months\n",
        "\n",
        "r_df = df.copy()\n",
        "\n",
        "X_r = r_df[['age','days_on_platform','income']]\n",
        "y_r = r_df['lifetime_value']\n",
        "\n",
        "\n",
        "X_train_r = X_r[:4000]\n",
        "y_train_r = y_r[:4000]\n",
        "\n",
        "X_test_r = X_r[1000:]\n",
        "y_test_r = y_r[1000:]\n",
        "\n",
        "\n",
        "Imp = IterativeImputer(max_iter=10, random_state = 0)\n",
        "Imp.fit(X_train_r)\n",
        "\n",
        "X_train_r = Imp.transform(X_train_r)\n",
        "X_test_r = Imp.transform(X_test_r)\n",
        "\n",
        "X_train_r = pd.DataFrame(X_train_r)\n",
        "X_train_r.columns = X_train_r.columns\n",
        "\n",
        "X_test_r = pd.DataFrame(X_test_r)\n",
        "X_test_r.columns = X_test_r.columns\n",
        "\n",
        "r_df = pd.concat([X_train_r,X_test_r],axis = 0)"
      ],
      "metadata": {
        "id": "gfW5JPUlbzlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nearest Neighbor Imputation\n"
      ],
      "metadata": {
        "id": "Gr6Qsorqb8VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
        "imputer.fit(X_train_r)\n",
        "X_train_k = imputer.transform(X_train_r)\n",
        "X_test_k = imputer.transform(X_test_r)\n",
        "\n",
        "y_train_k = y_train_r.copy()\n",
        "y_test_k = y_test_r.copy()"
      ],
      "metadata": {
        "id": "zVZ6aK9Hb8wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing duplicate rows"
      ],
      "metadata": {
        "id": "470U-ycZHma_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicate rows\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "#or\n",
        "\n",
        "# Drop duplicates\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "-kstcmibHmV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correcting inconsistent date formats"
      ],
      "metadata": {
        "id": "-1tnjbW1HmQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correcting inconsistent date formats\n",
        "data['date_of_purchase'] = pd.to_datetime(data['date_of_purchase'], errors='coerce')"
      ],
      "metadata": {
        "id": "HXNcybt2HmLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical Features"
      ],
      "metadata": {
        "id": "uBsXzZ3imrS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Hot Encoding\n"
      ],
      "metadata": {
        "id": "HQDWcS4qmyR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using pandas\n",
        "\n",
        "# Specify the columns you wish to one-hot encode\n",
        "categorical_columns = [\n",
        "    \"job\",\n",
        "    \"marital\"\n",
        "]\n",
        "\n",
        "# Perform the one-hot encoding\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# View the resulting DataFrame\n",
        "df_encoded"
      ],
      "metadata": {
        "id": "AjkQsehWnFzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# another option using Sklearn\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "# Specify the columns you wish to one-hot encode\n",
        "categorical_columns = [\"job\", \"marital\"]\n",
        "\n",
        "# Filter the DataFrame for the categorical features\n",
        "cat_features = df[categorical_columns]\n",
        "\n",
        "# Initialize the OneHotEncoder and fit it to the categorical features\n",
        "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "enc.fit(cat_features)\n",
        "\n",
        "# Use the transform method to one hot encode the categorical data and then convert it to a DataFrame\n",
        "enc_data = pd.DataFrame(\n",
        "    enc.transform(cat_features),\n",
        "    columns=enc.get_feature_names_out(categorical_columns)\n",
        ")\n",
        "\n",
        "# Join with the rest of the data and preview the DataFrame\n",
        "df_encoded = df.join(enc_data)\n",
        "df_encoded"
      ],
      "metadata": {
        "id": "k1BkSqpM2j-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Label Encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a copy of the original DataFrame\n",
        "df_encoded = df.copy()\n",
        "\n",
        "# Specify the column you wish to one-hot encode\n",
        "label_column = \"education\"\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Create a new column using the fit_transform method of the LabelEncoder\n",
        "df_encoded[label_column + \"_enc\"] = le.fit_transform(df_encoded[label_column])\n",
        "\n",
        "# Preview the original and encoded column\n",
        "df_encoded[[label_column, label_column + \"_enc\"]]"
      ],
      "metadata": {
        "id": "e8hVCNXu4xKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ordinal Encoding\n"
      ],
      "metadata": {
        "id": "HUhgsdUUm4Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "## Get the data we want to encode, convert to unique values\n",
        "data = np.asarray(df[['room_type']])\n",
        "\n",
        "# define ordinal encoding\n",
        "encoder = OrdinalEncoder()\n",
        "# transform data\n",
        "result = encoder.fit_transform(data)\n",
        "\n",
        "\n",
        "\n",
        "## Aggregate it back into original dataframe\n",
        "ord_encoded = pd.DataFrame(result)\n",
        "ord_encoded.columns = ['room_type_ord_encoded']\n",
        "\n",
        "ord_encoded_df = pd.concat([df, ord_encoded], axis = 1)\n",
        "\n",
        "ord_encoded"
      ],
      "metadata": {
        "id": "0HQm7fKjnglu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frequency Encoding\n"
      ],
      "metadata": {
        "id": "WZDNjP2Um5kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frequency_encoded = df.groupby(['neighbourhood']).size()\n",
        "\n",
        "frequency_encoded\n",
        "\n",
        "# replace the categories with these different frequencies\n",
        "df['neighbourhood'].apply(lambda x: frequency_encoded[x])"
      ],
      "metadata": {
        "id": "oX4cmsDCn3VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FrequencyEncoder:\n",
        "      \"\"\"\n",
        "    A simple frequency encoder for categorical variables.\n",
        "\n",
        "    This encoder replaces categorical values with their frequency counts based\n",
        "    on the distribution in the training dataset.\n",
        "    \"\"\"\n",
        "    def fit(self, train_df, column):\n",
        "        \"\"\"\n",
        "        Fits the encoder by storing the reference DataFrame and column.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        train_df : pandas.DataFrame\n",
        "            The training DataFrame containing the categorical column.\n",
        "        column : str\n",
        "            The name of the categorical column to encode.\n",
        "        \"\"\"\n",
        "        self.train_df = train_df\n",
        "        self.column = column\n",
        "\n",
        "    def transform(self, test_df, column):\n",
        "        \"\"\"\n",
        "        Transforms the test DataFrame by replacing categorical values with\n",
        "        their frequency counts from the training set.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        test_df : pandas.DataFrame\n",
        "            The DataFrame to be transformed.\n",
        "        column : str\n",
        "            The categorical column to encode.\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "        pandas.DataFrame\n",
        "            The transformed DataFrame with an added frequency-encoded column.\n",
        "        \"\"\"\n",
        "        frequency_encoded = self.train_df.groupby([self.column]).size()\n",
        "\n",
        "        col_name = column + '_freq'\n",
        "        test_df.loc[:,col_name] = test_df[column].apply(lambda x: frequency_encoded[x])\n",
        "        return test_df\n",
        "\n",
        "# frequency_encoding(df, column='neighbourhood')\n",
        "\n",
        "fe = FrequencyEncoder()\n",
        "fe.fit(df, column='neighbourhood')\n",
        "df_freq_enc = fe.transform(df, column='neighbourhood')\n",
        "\n",
        "df_freq_enc['neighbourhood_freq']"
      ],
      "metadata": {
        "id": "kitWHczEoG9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target Encoding  \n",
        "\n",
        " Rather than encode the number of instances a category occurs, we can encode the mean of our target variable, like this:"
      ],
      "metadata": {
        "id": "x2SDqs-em6tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(['neighbourhood']).mean()['price']"
      ],
      "metadata": {
        "id": "7ytU0nKlso3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TargetEncoder:\n",
        "    def fit(self, train_df, target_col, categ_col):\n",
        "        self.train_df = train_df\n",
        "        self.target_col = target_col\n",
        "        self.categ_col = categ_col\n",
        "\n",
        "    def transform(self, test_df, column = None):\n",
        "        if column is None:\n",
        "            column = self.categ_col\n",
        "\n",
        "        target_encoder = self.train_df.groupby([self.categ_col]).mean()[self.target_col]\n",
        "\n",
        "        df[self.categ_col].apply(lambda x: target_encoder[x])\n",
        "\n",
        "        col_name = column + '_target_enc'\n",
        "        test_df.loc[:,col_name] = test_df[column].apply(lambda x: target_encoder[x])\n",
        "        return test_df\n",
        "\n",
        "te = TargetEncoder()\n",
        "te.fit(df, target_col = 'price', categ_col = 'neighbourhood')\n",
        "\n",
        "te_df = te.transform(df)\n",
        "\n",
        "te_df"
      ],
      "metadata": {
        "id": "4-35d2F4ssse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probability Ratio Encoding  \n",
        "\n",
        "Probability Ratio Encoding is similar to target encoding. But rather than using the mean of the target, we're looking at the probability this category is going to be a positive label.\n"
      ],
      "metadata": {
        "id": "ApUUYipxm7pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProbabilityRatioEncoder:\n",
        "    def fit(self, train_df, categ_col, target_col):\n",
        "        self.train_df = train_df\n",
        "        self.categ_col = categ_col\n",
        "        self.target_col = target_col\n",
        "\n",
        "    def transform(self, test_df, constant = 0):\n",
        "        totals = self.train_df.groupby([self.categ_col]).size()\n",
        "        sums = self.train_df.groupby([self.categ_col]).sum()[self.target_col]\n",
        "\n",
        "        ratio_encoder = (sums+ constant)/totals\n",
        "\n",
        "        col_name = self.categ_col + '_prob_ratio'\n",
        "        test_df.loc[:,col_name] = test_df[self.categ_col].apply(lambda x: ratio_encoder[x])\n",
        "        return test_df\n",
        "\n",
        "# Example\n",
        "pre = ProbabilityRatioEncoder()\n",
        "\n",
        "pre.fit(df, 'neighbourhood','expensive')\n",
        "pre_df = pre.transform(df)\n",
        "\n",
        "pre_df"
      ],
      "metadata": {
        "id": "O5VJAURss9E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight of Evidence Encoder  \n",
        "\n",
        "Weight of Evidence encoding is similar to probability ratio encoding. The only difference, is we're applying a log transform on top of the probability ratio transformation:\n"
      ],
      "metadata": {
        "id": "WZxfm1Kzm8xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightofEvidenceEncoder:\n",
        "    def fit(self, train_df, categ_col, target_col):\n",
        "        self.train_df = train_df\n",
        "        self.categ_col = categ_col\n",
        "        self.target_col = target_col\n",
        "\n",
        "    def transform(self, test_df, constant = 0):\n",
        "        totals = self.train_df.groupby([self.categ_col]).size()\n",
        "        sums = self.train_df.groupby([self.categ_col]).sum()[self.target_col]\n",
        "\n",
        "        woe_encoder = np.log((sums+ constant)/totals)\n",
        "\n",
        "        col_name = self.categ_col + '_woe'\n",
        "        test_df.loc[:,col_name] = test_df[self.categ_col].apply(lambda x: woe_encoder[x])\n",
        "        return test_df\n",
        "\n",
        "woe = WeightofEvidenceEncoder()\n",
        "\n",
        "woe.fit(df, 'neighbourhood','expensive')\n",
        "woe_df = woe.transform(df)\n",
        "\n",
        "woe_df"
      ],
      "metadata": {
        "id": "Z7wzLkhStN71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binning  \n",
        "\n",
        "The last technique is called binning. This is where we take a continuous variable and bin them into different buckets, thus, transforming this continuous variable into a categorical variable\n"
      ],
      "metadata": {
        "id": "1MlaPOxHm-HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[:,'last_review'] = pd.to_datetime(df['last_review'])\n",
        "\n",
        "def reviews_bin(x):\n",
        "    if x < 50:\n",
        "        return 'less_50'\n",
        "    if x >= 50 and x < 100:\n",
        "        return '50_to_100'\n",
        "    if x >= 100 and x < 150:\n",
        "        return '100_to_150'\n",
        "    if x >= 150 and x < 200:\n",
        "        return '150_to_200'\n",
        "    if x >= 200:\n",
        "        return '200_plus'\n",
        "\n",
        "df['number_of_reviews'].map(reviews_bin)"
      ],
      "metadata": {
        "id": "f7LxPpcom3fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuous Features\n"
      ],
      "metadata": {
        "id": "gxjnyVwFtpJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling  \n",
        "\n",
        "Feature scaling is important for we are using models with a distance metric. If our features are of different scales, they can be overcompensated for in the models.\n"
      ],
      "metadata": {
        "id": "1F20oApptuCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Absolute Max Scaling\n"
      ],
      "metadata": {
        "id": "nDC1KJQFuAlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_example.drop('price',axis =1 )\n",
        "y = df_example[['price']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "nnwhdxjvukUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "#Scale data\n",
        "df_am = MaxAbsScaler().fit_transform(X_train)\n",
        "\n",
        "#convert to dataframe to see table\n",
        "df_am = pd.DataFrame(df_am, columns = X_train.columns)\n",
        "\n",
        "#obvious problems with outliers regarding price & odometer"
      ],
      "metadata": {
        "id": "B5m_uaP9ufCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MinMax Scaling  \n",
        "\n",
        "Between [0 and 1]\n",
        "\n"
      ],
      "metadata": {
        "id": "HpmKieV7uCIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Be carefule of the data leakage\n",
        "df_min_max = MinMaxScaler().fit_transform(X_train)\n",
        "df_min_max = pd.DataFrame(df_min_max, columns = X_train.columns)\n",
        "\n",
        "\n",
        "# fit and transform in different steps\n",
        "\n",
        "# load the packages\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Step 1: Create a dataset\n",
        "data = np.array([[10], [20], [30], [40], [50]], dtype=float)\n",
        "\n",
        "# Step 2: Initialize MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))  # Scale between 0 and 1\n",
        "\n",
        "# Step 3: Fit the scaler to the data (calculates min and max)\n",
        "scaler.fit(data)\n",
        "\n",
        "# Step 4: Transform the data (scales using min-max formula)\n",
        "scaled_data = scaler.transform(data)\n",
        "\n",
        "# Step 5: Print results\n",
        "print(\"Original Data:\\n\", data)\n",
        "print(\"Scaled Data:\\n\", scaled_data)\n",
        "\n",
        "# Checking the min_ and scale_ attributes\n",
        "print(\"Data Min:\", scaler.data_min_)\n",
        "print(\"Data Max:\", scaler.data_max_)\n",
        "print(\"Scaling Factor:\", scaler.scale_)"
      ],
      "metadata": {
        "id": "4m8QpfNYvh4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Z-Score Normalization (Standard Scaler)\n"
      ],
      "metadata": {
        "id": "6EMITpjJuD61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "df_std = X_train.copy()\n",
        "#only scale numeric varaibles in this case rather than the dummy variables for categories\n",
        "df_std.loc[:,['car_age','odometer']] = StandardScaler().fit_transform(df_std.loc[:, ['car_age','odometer']])\n",
        "df_std"
      ],
      "metadata": {
        "id": "V4hag1Zivkxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Robust Scaler\n"
      ],
      "metadata": {
        "id": "Aun7nUoSuFH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "df_rob = X_train.copy()\n",
        "#only scale numeric varaibles in this case rather than the dummy variables for categories\n",
        "df_rob.loc[:,['car_age','odometer']] = RobustScaler().fit_transform(df_rob.loc[:, ['car_age','odometer']])\n",
        "df_rob"
      ],
      "metadata": {
        "id": "LX3CnDFGvnbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformations  \n",
        "\n",
        "A data transformation is the process of using a math expression to change the structure of our data. As we mentioned before, some models need data to fit a specific type of distribution for them to produce optimal results. Unfortunately, the data we get in the real world, doesn’t always fit the distributions our models call for.\n"
      ],
      "metadata": {
        "id": "mnUnZjRSuGV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logarithmic\n"
      ],
      "metadata": {
        "id": "kcOEcR7luIDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "def log_transform(x):\n",
        "    return np.log(x + 1)\n",
        "\n",
        "transformer_log = FunctionTransformer(log_transform)\n",
        "transformed_log = transformer_log.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "ZcmxXGCGwISq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_logp = FunctionTransformer(log_transform)\n",
        "transformed_logp = transformer_logp.fit_transform(y_train)"
      ],
      "metadata": {
        "id": "TqFAVG6hwJwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Square Root\n"
      ],
      "metadata": {
        "id": "UgdPm-F0uJI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sqrt_transform(x):\n",
        "    return np.sqrt(x)\n",
        "\n",
        "transformer_sqrt = FunctionTransformer(sqrt_transform)\n",
        "transformed_sqrt = transformer_sqrt.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "Unp-8l8xwSVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_sqrtp = FunctionTransformer(sqrt_transform)\n",
        "transformed_sqrtp = transformer_sqrtp.fit_transform(y_train)"
      ],
      "metadata": {
        "id": "5UHT_1lnwUQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exponential\n"
      ],
      "metadata": {
        "id": "bn33RaJVuKL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exp_transform(x):\n",
        "    return np.exp(x)\n",
        "\n",
        "transformer_exp = FunctionTransformer(exp_transform)\n",
        "\n",
        "## In our dataset, car age may be something we want to magnify\n",
        "transformed_exp = X_train.copy()\n",
        "\n",
        "transformed_exp['car_age'] = transformer_exp.fit_transform(transformed_exp['car_age'])"
      ],
      "metadata": {
        "id": "uQQCiuOlwe8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Box-Cox  \n",
        "\n",
        "The Box-Cox transformation is a transformation that helps your dataset follow a normal distribution. Typically, we use Box-Cox transformation when our dataset is not normal, but close to being normal. When we want to either run tests or generate significance from our dataset, Box-Cox transformation is a good option to transform our target variable so it resembles a normal distribution.\n",
        "\n",
        "Box-Cox aggregates multiple power transformers into a single transformer. You use lambda to adjust the transformation. Lambda varies from -5 to 5. If we set lambda equal to zero, it becomes simply a log transformation.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jzMStjiuLSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Redo the pipeline for this example\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Clip Outliers\n",
        "df_example = df_example[df_example['price'] < np.percentile(df_example['price'], 95)]\n",
        "\n",
        "## Remove prices that are 0 to make notebook work\n",
        "df_example = df_example[df_example['price'] > 0].copy()\n",
        "\n",
        "X = df_example.drop('price',axis =1 )\n",
        "y = df_example[['price']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "-Q4P8bGTwkxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import boxcox\n",
        "\n",
        "boxcox_y_train = boxcox(y_train['price'], lmbda = None)\n",
        "\n",
        "plt.hist(boxcox_y_train[0])"
      ],
      "metadata": {
        "id": "XvzX7bNZxPBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lambda Parameter {0}\".format(boxcox_y_train[1]))"
      ],
      "metadata": {
        "id": "k7NKxF8zxRSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interaction Features  \n",
        "\n",
        "While the interaction could be with the variable itself, we can also take differences, ratios and mutliples of two or more variables.\n"
      ],
      "metadata": {
        "id": "6IKHO4vduNAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arethmetic Interaction\n"
      ],
      "metadata": {
        "id": "iyHt5Wf-uN_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first example of getting car's age:\n",
        "df['car_age'] = df['year'].max() - df['year']\n",
        "\n",
        "#let's look at price per mile. This could be a good way to normalize across different car brands\n",
        "df['price_per_mile'] = df['price']/ df['odometer']"
      ],
      "metadata": {
        "id": "8TvxaQ_Sxf8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binning\n"
      ],
      "metadata": {
        "id": "ufMDlek1uO1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create warranty bin > 50,000 miles\n",
        "# we are using a lambda function here. This lets us write a function without defining it\n",
        "# we are also using a ternary operator which is an if, else statement in a single line. (explained in the full video)\n",
        "\n",
        "df['warranty_miles'] = df['odometer'].apply(lambda x: 0 if (x > 50000 or np.isnan(x)) else 1)\n",
        "df['warranty_age'] = df['car_age'].apply(lambda x: 0 if (x > 5 or np.isnan(x)) else 1)\n",
        "\n",
        "\n",
        "# We can also combine these together in a single statement by defining a function.\n",
        "def warranty(miles, age):\n",
        "    if (miles > 50000 or age > 5):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "df['warranty'] = df.apply(lambda x: min(x.warranty_miles,x.warranty_age), axis=1)\n"
      ],
      "metadata": {
        "id": "szfnLp-gxvFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dealing with Imbalanced Data"
      ],
      "metadata": {
        "id": "4bvlqLFJoW4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diagnosis"
      ],
      "metadata": {
        "id": "zzi1hTh_ojs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.target.value_counts()"
      ],
      "metadata": {
        "id": "_5AF8shsoaux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see imblance in data majority is not looking for a job change (0)\n",
        "y_train.value_counts().plot.bar()"
      ],
      "metadata": {
        "id": "a0tu-YRyoooi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Oversampling"
      ],
      "metadata": {
        "id": "VUcPi8ueoznX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "o_smpl = RandomOverSampler(random_state = 42)\n",
        "\n",
        "X_o_smpl, y_o_smpl = o_smpl.fit_resample(X_train,y_train)\n",
        "\n",
        "y_o_smpl.value_counts().plot.bar()"
      ],
      "metadata": {
        "id": "ByeqK1vDo1wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Undersampling"
      ],
      "metadata": {
        "id": "w9yEqMQoo56Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "u_smpl = RandomUnderSampler(random_state = 42)\n",
        "\n",
        "X_u_smpl, y_u_smpl = u_smpl.fit_resample(X_train,y_train)"
      ],
      "metadata": {
        "id": "uWiMm2Fxo8s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot\n",
        "y_u_smpl.value_counts().plot.bar()"
      ],
      "metadata": {
        "id": "wC_rgnw0pA99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthetic Minority Oversampling (SMOTE)"
      ],
      "metadata": {
        "id": "ReNH2wUTpE95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state = 42)\n",
        "\n",
        "X_smote, y_smote = smote.fit_resample(X_train,y_train)\n",
        "\n",
        "#plot\n",
        "y_smote.value_counts().plot.bar()"
      ],
      "metadata": {
        "id": "lOtD_aGRpH9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Borderline SMOTE"
      ],
      "metadata": {
        "id": "kHVsXtPmqpaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "\n",
        "bsmote = BorderlineSMOTE(random_state = 42)\n",
        "\n",
        "X_bsmote, y_bsmote = bsmote.fit_resample(X_train,y_train)\n",
        "\n",
        "#plot\n",
        "y_smote.value_counts().plot.bar()"
      ],
      "metadata": {
        "id": "qMbHUcftqo1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adaptive Synthetic Oversampling (ADASYN)"
      ],
      "metadata": {
        "id": "gCBuS3BPqxUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "adasyn = ADASYN(random_state = 42)\n",
        "\n",
        "X_ada, y_ada = adasyn.fit_resample(X_train,y_train)\n",
        "\n",
        "# plot\n",
        "y_ada.value_counts().plot.bar()\n"
      ],
      "metadata": {
        "id": "QmOgZhi-qyw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge datasets"
      ],
      "metadata": {
        "id": "mlEf901pmi_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging datasets on a common key"
      ],
      "metadata": {
        "id": "gx54EoRMHmGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging datasets on a common key 'customer_id'\n",
        "merged_data = pd.merge(data1, data2, on='customer_id', how='inner')"
      ],
      "metadata": {
        "id": "-3GEjIduHmBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joining on column(s) with the same name"
      ],
      "metadata": {
        "id": "cDxLFeRsNYY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "join1 = pd.merge(\n",
        "    left=df1,  # Specify DataFrame on the left to merge\n",
        "    right=df2,  # Specify DataFrame on the right to merge\n",
        "    how=\"left\",  # Choose 'inner', 'outer', 'cross' 'left' or 'right'\n",
        "    on=[\"uid\"],  # List of column(s) to merge on (these must exist in both DataFrames)\n",
        "    indicator=True,  # When true, adds “_merge” column with source of each row\n",
        ")\n",
        "\n",
        "print(\"Number of rows & columns:\", join1.shape)\n",
        "join1.head()"
      ],
      "metadata": {
        "id": "p7_hoB95NYRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joining on column(s) with different names"
      ],
      "metadata": {
        "id": "up4Vq-nYNhUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "join2 = pd.merge(\n",
        "    left=df1,  # Specify DataFrame on the left to merge\n",
        "    right=df2,  # Specify DataFrame on the right to merge\n",
        "    how=\"inner\",  # Choose 'inner', 'outer', 'cross' 'left' or 'right'\n",
        "    left_on=[\"reg_date\", \"uid\"],  # List of column(s) to merge on in the left DataFrame\n",
        "    right_on=[\"date\", \"uid\"],  # List of column(s) to merge on in the right DataFrame\n",
        "    indicator=True,  # When true, adds “_merge” column with source of each row\n",
        ")\n",
        "\n",
        "print(\"Number of rows & columns:\", join2.shape)\n",
        "join2.head()"
      ],
      "metadata": {
        "id": "xsdIFMa0Nf6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling numeric data"
      ],
      "metadata": {
        "id": "jcbx8xfRHl7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Creating a manual dataset\n",
        "data = pd.DataFrame({\n",
        "    'category': ['A', 'B', 'A', 'C', 'B'],\n",
        "    'numeric_column': [10, 15, 10, 20, 15]\n",
        "\t})\n",
        "\n",
        "# Scaling numeric data\n",
        "scaler = StandardScaler()\n",
        "data['scaled_numeric_column'] = scaler.fit_transform(data[['numeric_column']])"
      ],
      "metadata": {
        "id": "VnhiYOWlHl2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## one-hot encoding"
      ],
      "metadata": {
        "id": "CT_QgjJlHlxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a manual dataset\n",
        "data = pd.DataFrame({\n",
        "    'category': ['A', 'B', 'A', 'C', 'B'],\n",
        "    'numeric_column': [10, 15, 10, 20, 15]\n",
        "\t})\n",
        "\n",
        "# Encoding categorical variables using one-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded_data = pd.DataFrame(encoder.fit_transform(data[['category']]),\n",
        "                            columns=encoder.get_feature_names_out(['category']))\n",
        "\n",
        "# Concatenating the encoded data with the original dataset\n",
        "data = pd.concat([data, encoded_data], axis=1)"
      ],
      "metadata": {
        "id": "E1xRJM2eHltI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Features selection"
      ],
      "metadata": {
        "id": "VjDC-l5tJyeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = []\n",
        "for column in df.columns:\n",
        "    if column != 'output':        # Change 'output' with the output name variable\n",
        "        features.append(column)\n",
        "X = df[features]\n",
        "Y = df['output']                   # Change 'output' with the output name variable"
      ],
      "metadata": {
        "id": "tKiKclTeJy9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize correlation"
      ],
      "metadata": {
        "id": "3FMmxVzLHloM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# Upload your data as CSV and load as data frame\n",
        "df = pd.read_csv('mpg.csv')\n",
        "df\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr = df.corr(method = 'pearson')\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "fig, ax = plt.subplots(figsize=(11, 9))                    # Set figure size\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask\n",
        "sns.heatmap(corr,\n",
        "            mask = mask,\n",
        "            cmap = cmap,\n",
        "            vmax = 1,                                      # Set scale min value\n",
        "            vmin = -1,                                     # Set scale min value\n",
        "            center = 0,                                    # Set scale min value\n",
        "            square = True,                                 # Ensure perfect squares\n",
        "            linewidths = 1.5,                              # Set linewidth between squares\n",
        "            cbar_kws = {\"shrink\": .9},                     # Set size of color bar\n",
        "            annot = True                                   # Include values within squares\n",
        "           );\n",
        "\n",
        "plt.xticks(rotation=45)                                    # Rotate x labels\n",
        "plt.yticks(rotation=45)                                    # Rotate y labels\n",
        "# plt.xlabel('X Axis Title', size=20)                      # Set x axis title\n",
        "# plt.ylabel('Y Axis Title', size=20)                      # Set y axis title\n",
        "plt.title('Diagonal Correlation Plot', size=30, y=1.05);   # Set plot title and position"
      ],
      "metadata": {
        "id": "LdPiKDgBHljF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling"
      ],
      "metadata": {
        "id": "nU1W5X8RHlek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "for column in X.columns:\n",
        "    feature = np.array(X[column]).reshape(-1,1)\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(feature)\n",
        "    feature_scaled = scaler.transform(feature)\n",
        "    X[column] = feature_scaled.reshape(1,-1)[0]"
      ],
      "metadata": {
        "id": "Q47YkDfOHlaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rn_8RfpHHkCg"
      }
    }
  ]
}